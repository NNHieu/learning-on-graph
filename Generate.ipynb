{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.tokenizer import GPT2NumeralTokenizer\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from src.data import build_tokenize_function\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    DataCollatorWithPadding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"outputs/star_graph/dpo_gpt2_12x6x384/checkpoint-1500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a base model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,    \n",
    "    # device_map={\"\": accelerator.process_index},\n",
    "    # torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kappa/miniconda3/envs/handbook/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2NumeralTokenizer(\n",
    "                50,\n",
    "                padding_side='left'\n",
    "            )\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200000/200000 [00:32<00:00, 6109.18 examples/s]\n",
      "Map: 100%|██████████| 20000/20000 [00:03<00:00, 6112.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = datasets.load_dataset(\"nnheui/star-d2_p5_n50\")\n",
    "def tokenize_function(examples, text_column_name=\"text\"):\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    attention_mask = []\n",
    "    for line in examples[text_column_name]:\n",
    "        prefix, target = line.strip().split('=')\n",
    "        prefix += \"=\"\n",
    "        prefix = tokenizer.encode(prefix)\n",
    "        target = tokenizer.encode(target)\n",
    "        input_ids.append(prefix)\n",
    "        labels.append(target)\n",
    "        attention_mask.append([1] * len(prefix))\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"targets\": labels,\n",
    "    }\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47,46|31,29|29,23|43,5|23,47|41,43|44,41|31,44/31,46=\n",
      "[31, 29, 23, 47, 46]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_datasets[\"train\"][1][\"input_ids\"]))\n",
    "print(tokenized_datasets[\"train\"][1][\"targets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"].select(list(range(10000))), shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer), batch_size=256)\n",
    "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49, 13, 50,  ..., 46, 22, 51],\n",
       "        [47, 46, 50,  ..., 31, 46, 51],\n",
       "        [ 7,  6, 50,  ...,  7,  1, 51],\n",
       "        ...,\n",
       "        [15, 20, 50,  ..., 15, 32, 51],\n",
       "        [ 4, 25, 50,  ..., 32, 36, 51],\n",
       "        [ 1, 46, 50,  ..., 18, 46, 51]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'targets': tensor([[46, 35, 32, 42, 22],\n",
       "        [31, 29, 23, 47, 46],\n",
       "        [ 7,  6, 31, 14,  1],\n",
       "        ...,\n",
       "        [15, 20, 47, 12, 32],\n",
       "        [32, 23, 22, 45, 36],\n",
       "        [18, 32, 16,  1, 46]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(eval_dataloader, model, top_k=None):\n",
    "    model.eval()\n",
    "    all_generated = []\n",
    "    all_labels = []\n",
    "    all_token_probs = []\n",
    "    all_topk_idx = []\n",
    "    all_topk_value = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        labels = batch['targets']\n",
    "        num_target_tokens = labels.shape[1]\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids = batch[\"input_ids\"].to(\"cuda:2\"),\n",
    "                attention_mask = batch[\"attention_mask\"].to(\"cuda:2\"),\n",
    "                max_new_tokens=num_target_tokens,\n",
    "                do_sample=False,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True, \n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            generated_ids = outputs.sequences[:,-num_target_tokens:]\n",
    "            logits = outputs.logits\n",
    "            logits = torch.stack(logits, dim=1)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            token_probs = probs.gather(-1, generated_ids.unsqueeze(-1)).squeeze(-1)\n",
    "            generated_ids = generated_ids[:, -num_target_tokens:].cpu()\n",
    "\n",
    "            topk = probs[:, 0].topk(3)\n",
    "        all_generated.append(generated_ids)\n",
    "        all_labels.append(labels)\n",
    "        all_token_probs.append(token_probs)\n",
    "        all_topk_idx.append(topk.indices)\n",
    "        all_topk_value.append(topk.values)\n",
    "    return {\n",
    "        \"generated\": torch.cat(all_generated, dim=0),\n",
    "        \"labels\": torch.cat(all_labels, dim=0),\n",
    "        \"token_probs\": torch.cat(all_token_probs, dim=0),\n",
    "        \"topk_idx\": torch.cat(all_topk_idx, dim=0),\n",
    "        \"topk_value\": torch.cat(all_topk_value, dim=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:2\")\n",
    "outputs = generate(train_dataloader, model)\n",
    "# outputs = generate(val_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3577)\n",
      "tensor([1.0000, 0.4587, 0.4205, 0.3994, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "processed_all_generated = outputs['generated']\n",
    "processed_all_labels = outputs[\"labels\"]\n",
    "\n",
    "mask = (processed_all_labels != tokenizer.eos_token_id)\n",
    "correct = (processed_all_generated == processed_all_labels) * mask\n",
    "print((correct.sum(dim=1) == mask.sum(dim=1)).float().mean())\n",
    "print(correct.sum(dim=0) / (mask.sum(dim=0) + 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38,4|26,29|32,45|45,16|2,32|23,26|29,38|23,2/23,16=\n",
      "tensor([23, 26, 29, 38, 16]) tensor([23,  2, 32, 45, 16])\n",
      "tensor([0.9399, 0.4061, 0.6869, 0.6768, 0.9393], device='cuda:2')\n",
      "tensor([23, 37, 26], device='cuda:2')\n",
      "tensor([9.3989e-01, 6.3950e-04, 6.1606e-04], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][idx][\"input_ids\"]))\n",
    "print(processed_all_generated[idx], processed_all_labels[idx])\n",
    "\n",
    "print(outputs['token_probs'][idx])\n",
    "print(outputs[\"topk_idx\"][idx])\n",
    "print(outputs[\"topk_value\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
